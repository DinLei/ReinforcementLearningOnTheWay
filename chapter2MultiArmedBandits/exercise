Exercise 2.1 In ε-greedy action selection, for the case of two actions and ε = 0.5,
what is the probability that the greedy action is selected?


Exercise 2.2: Bandit example Consider a multi-armed bandit problem with k = 4
actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm
using ε-greedy action selection, sample-average action-value estimates, and initial
estimates of Q1(a) = 0, ∀a. Suppose the initial sequence of actions and rewards is
A1 = 1, R1 = 1, A2 = 2, R2 = 1, A3 = 2, R3 = 2, A4 = 2, R4 = 2, A5 = 3, R5 = 0.
On some of these time steps the ε case may have occurred, causing an action to be
selected at random. On which time steps did this definitely occur? On which time
steps could this possibly have occurred?


Exercise 2.3 In the comparison shown in Figure 2.2, which method will perform best
in the long run in terms of cumulative reward and cumulative probability of selecting
the best action? How much better will it be? Express your answer quantitatively.


Exercise 2.4 If the step-size parameters, αn , are not constant, then the estimate
Qn is a weighted average of previously received rewards with a weighting different
from that given by (2.6). What is the weighting on each prior reward for the general
case, analogous to (2.6), in terms of the sequence of step-size parameters?


Exercise 2.5 (programming) Design and conduct an experiment to demonstrate
the difficulties that sample-average methods have for nonstationary problems. Use a
modified version of the 10-armed testbed in which all the q∗(a) start out equal and
then take independent random walks (say by adding a normally distributed increment
with mean zero and standard deviation 0.01 to all the q∗(a) on each step). Prepare
plots like Figure 2.2 for an action-value method using sample averages, incrementally
computed, and another action-value method using a constant step-size parameter,
α = 0.1. Use ε = 0.1 and longer runs, say of 10,000 steps.