K型武装强盗问题

1-1. k种行为（动作），每采取一种行为都会得到相应的奖励，这种奖励就被定义为行为价值（action value）;
1-2. 如果知道所有行为价值，自然能选择带来最大价值的行为；一般情况下是不知道的，所以得有只能采用估计值;
1-3. 如果每次都根据历史信息选择最大估计值的行为这就叫「开发」（exploiting），这样的行为叫做贪婪行为（greedy action）;
     但是，如果选择非贪婪的估计，这就叫做「探索」（exploring）;
     有探索行为可能会降低短期收益，但是很可能拥有更高的长期收益;

2-1. 行为价值估计：行为a的价值就是历史中采取行为a带来的平均奖励;(P49,P53)
2-2. 贪婪行为选择-Greedy Action Selection
2-3. 近似贪婪行为选择-Epsilon-greedy Action Selection

2-4. 非平稳问题(见书P55)

2-5. 乐观初始值(Optimistic Initial Values)
     1)较大的初始值能在开始的时候引导更多的探索行为;
     2)但是这种技巧仅适合在平稳问题，因为这种技巧带来的探索冲动本质上是暂时的，如果任务改变了，需要重新创建探索需求，这种方法就没用了;
     3)任何以特殊方式关注初始状态的方法都不太可能有助于一般的非平稳情况;

2-6. 上置信界行为选择(UCB: upper confidence bound)
     1)近似贪婪行为选择方法对非贪婪的行为选择是没用准则的，但最好是根据它们实际的最优潜力来做出选择，同时考虑对非贪婪的估计值与最大值
     之间的接近程度和估计的不确定性;
     2)公式见书P58;
     3)左边的平方根项是一种不确定性的度量或者是对行为a的价值的方差;
     4)项数c就代表了置信度的等级;
     5)公式中的最大值就代表了行为a的价值的上确界;

2-7. 梯度强盗算法(Gradient Bandit Algorithm)
     1)对行为a的数值上的偏好H_t(a);
     2)行为a的偏好越大越可能被选择到，但是这样的偏好在奖励上面没用解释，它仅是对行为的偏好;
     3)行为a被选择的概率与其偏好服从softmax分布;P59
     4)P60公式推导;

2-8. 关联查找(Associative Search)
     1)上述都只考虑了非关联任务：不需要将不同行为与不同情景联系起来;
     2)在非关联任务中，学习者要么是在平稳状态任务中单次最佳动作，要么是在非平稳状态下追踪最佳动作;
     3)假如真实的行为价值是随机变化的，除非变化较慢，否则非平稳的追踪方法是无法解决这类问题的。但是如果获得除动作价值以外的其他线索，
     那么就能根据这样的信息去对最佳行动作出推断;